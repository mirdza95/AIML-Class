{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64de9dd",
   "metadata": {},
   "source": [
    "Metrics to use for testing performance of classifiers\n",
    "\n",
    "\n",
    "- I want you people to assess, and ask me questions related to the ML process so far. \n",
    "- complicated, or tough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e1d6d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbed11ed",
   "metadata": {},
   "source": [
    "Classification Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f1839",
   "metadata": {},
   "source": [
    "- Accuracy = all the time - not that usable - imbalance, it will not show you true performance\n",
    "- Precision = when FP are crucial =  TP / TP + FP - \n",
    "- Recall = when FN are crucial = TP / TP + FN \n",
    "- F1 Score = to check performance in general = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723ebb8",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "- Metrics - Precision , Recall , F1 score\n",
    "- Each class will have its own metrics\n",
    "- Macro avg: averages all the metrics for the individual classes\n",
    "- Weighted avg: considers the weight of each class as well, especially useful, with class imbalane\n",
    "- Accuracy - for the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63df698",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
